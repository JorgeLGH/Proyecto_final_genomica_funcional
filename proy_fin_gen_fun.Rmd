---
title: "Proyecto final Genómica Funcional"
author: "Gómez Hernández Jorge Luis, Rojas García Luis Andrés"
date: "5/28/2021"
output: 
  html_document: 
    theme: cerulean
---


Primero, señalamos el directorio de trabajo donde se va a llevar todo a cabo y cargamos la librería principal que se usará.
```{r}
setwd("~/R/Genomica_funcional/pro_fin_genfun/") #cambiar para PROPIO directorio de trabajo
library(dada2)
```
Ahora se define la ruta en la cual están los archivos FASTQ que se usarán
```{r}
ruta<-"~/R/Genomica_funcional/pro_fin_genfun/fastq_pro_fin_suelo/" #debe de cambiarse segpun el usuario y su localización de sus archivos
head(list.files(ruta)) #manda los nombres de todos los archivos que hay en esa ruta
```
El siguiente paso es definir qué secuencias son las reverse y las forward, esto porque el experimento cuenta con secuencias pareadas. En este caso particular, los archivos con *_1* son la secuencia *forward*, mientras que los archivos con *_2* son las secuencias *reverse*.
```{r}
#estas líneas dividen los 
forward<-sort(list.files(ruta, pattern="_1.fastq", full.names = TRUE))
reverse<-sort(list.files(ruta, pattern="_2.fastq", full.names = TRUE))
sample.names <- sapply(strsplit(basename(forward), "_"), `[`, 1) #con esta línea tenemos los nombres de las muestras sin la clasificación de forward y reverse. Separa y selecciona según la función sapply, tomamos el primer fragmento de la separación
```
# Calidad Phred
En esta parte se usará la calidad Phred para poder realizar, mas adelante, los cortes en los reads según lo que se pueda observar con las gráficas que se basan en este índice de calidad de los reads.
```{r}
#visualización de la calidad de Phred de las tres primeras secuencias tanto de forward como de reverse
plotQualityProfile(forward[1:3])
plotQualityProfile(reverse[1:3])
```
# Filtrado y trimming de los reads
En esta parte se hace el filtrado de los reads con los cortes arbitrarios basados con la visualización de la calidad Phred.
```{r}
# Colocar los archivos en un archivo llamado filtered y que contenga los forward y reverse compresos en gz
filtFs <- file.path(ruta, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(ruta, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
#asignamos los nombres a los archivos filtrados
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```
Ahora ya sigue el corte y la formación de los archivos filtrados.
```{r}
#aquí se genera el archivo como tal, cada uno de los archivos "filtered" de la parte anterior ahora existe con la información obtenida aquí, ya son las secuencias filtradas con los cortes arbitrarios que se desearon.
out <- filterAndTrim(forward, filtFs, reverse, filtRs, truncLen=c(200,140),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, 
              compress=TRUE, multithread=F)
#se hizo el corte en las bases 200 y 140 respectivamente para poder incluir solo aquellas secuencias que sin duda son de buena calidad
head(out)
```
# Tasas de error
En esta parte se usa *machine learning* con la finalidad de poder dar el parámetro de error, se van alternando estimaciones de error inferido de la muestra y la estimación de la razón de errores hasta que convergen
```{r}
#errF<-learnErrors(filtFs, multithread=TRUE)
#save(errF,file="~/R/Genomica_funcional/pro_fin_genfun/errF.RData")
#se carga el arhivo para un flujo más rápido
load("errF.RData")

#mismo proceso para los de reverse
#errR<-learnErrors(filtRs, multithread=TRUE)
#save(errR,file="~/R/Genomica_funcional/pro_fin_genfun/errR.RData")
load("errR.RData")
#esta parte los puntos son las tasas de error observadas; la línea roja es la tasa de error esperada
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```
# Inferencia de la muestra

```{r}
#testa función va a remover los errores de la secuenciación, por eso es que se calculó el error previamente, anos ayudará a describir la comunidad
#dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
#save(dadaFs,file="~/R/Genomica_funcional/pro_fin_genfun/dadaFs.RData")
#se guaradó y cargó el objeto para agilizar el proceso
load("dadaFS.RData")
#dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
#save(dadaRs,file="~/R/Genomica_funcional/pro_fin_genfun/dadaRs.RData")
#mismo proceso que con el objeto anterior
load("dadaRS.RData")
```
# Unir las secuencias *paired reads*
En esta sección unimos las secuencias *forward* y las *reverse* de las muestras obtenidas.
```{r}
#
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]])
```

# Construir la tabla de secuencias

```{r}
seqtab <- makeSequenceTable(mergers)#esta función construye una tabla de secuencias, análoga a una tabla de OTU's, a partir de la lista de las muestras
dim(seqtab)#hay un total de 194 muestras, y se tienen en total 12757 secuencias
```

# Remover quimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)#después de quitar el ruido, se deben de quitar las quimeras que quedaron. las quimeras son secuencias que fueron unidas de manera incorrecta, se genera una tabla ya sin quimeras
dim(seqtab.nochim)#del total de las 194 muestras, quitando las quimeras, quedan 2678 secuencias
sum(seqtab.nochim)/sum(seqtab)#si está bien hehco, la mayoría de las secuencias deben de quedar y el valor de esta operación debe de ser cercano a uno. En este caso el valor es de 0.9987378
```

# Verificar el número de lecturas después del pipeline

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))#esto permite ver la cantidad de reads y secuencias únicas con las que empezamos y cuántas se han retirado cada paso. Parece estar bien ya que no se han perdido muchos reads
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
# Asignar taxonomía

Para asignar las taxonomías se usan los archivos silva_nr_v132_train_set.fa.gz y RDP_LSU_fixed_train_set_v2.fa.gz
Para la taxonomía de hongos, consultar repositorio de Github para ver cómo se obtuvo todo.
